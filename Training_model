import numpy as np # used to for array structres
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Conv2D,Input,Conv2DTranspose,Activation,BatchNormalization,ReLU,Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint #Callback to save the Keras model or model weights at some frequency
from tensorflow.keras.datasets import cifar100

#10 - This is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 10 categories.
#100 - This is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 100 fine-grained classes that are grouped into 20 coarse-grained classes.

#Conv2D - convolution 2D layer , input - used to input data to layers , conv2dtranspose - Deconvolution
#Activation - activates a layer , #batchnormaliztion - Layer that normalizes its inputs i.e mean and variance of inputs
#ReLU - Rectified Linear Unit activation , concate -Layer that concatenates a list of inputs


(train_data_clean, _), (test_data_clean, _) = cifar100.load_data(label_mode='fine')

train_data_clean = train_data_clean.astype('float32') / 255.
test_data_clean = test_data_clean.astype('float32') / 255.

#It is most common to use 32-bit precision when training a neural network, so at one point the training data will have to be converted to 32 bit floats.
#255, this is the maximum value of a byte (the input feature's type before the conversion to float32),
# so this will ensure that the input features are scaled between 0.0 and 1.0


def add_noise_and_clip_data(data):
    noise = np.random.normal(loc=0.0, scale=0.1, size=data.shape)
    data = data + noise
    data = np.clip(data, 0., 1.)
    return data

train_data_noisy = add_noise_and_clip_data(train_data_clean)
test_data_noisy = add_noise_and_clip_data(test_data_clean)

for i in(1,100,234):
    idx = i
    plt.subplot(1,2,1)
    plt.imshow(train_data_clean[idx])
    plt.title('Original image')
    plt.subplot(1,2,2)
    plt.imshow(train_data_noisy[idx])
    plt.title('Image with noise')
    plt.show()

def conv_block(x, filters, kernel_size, strides):
    x = Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding='same')(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    return x

def deconv_block(x, filters, kernel_size):
    x = Conv2DTranspose(filters=filters,kernel_size=kernel_size,strides=2,padding='same')(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    return x

def denoising_autoencoder():
    dae_inputs = Input(shape=(32, 32, 3), name='dae_input')
    conv_block1 = conv_block(dae_inputs, 32, 3)
    conv_block2 = conv_block(conv_block1, 64, 3)
    conv_block3 = conv_block(conv_block2, 128, 3)
    conv_block4 = conv_block(conv_block3, 256, 3)
    conv_block5 = conv_block(conv_block4, 256, 3, 1)

    deconv_block1 = deconv_block(conv_block5, 256, 3)
    merge1 = Concatenate()([deconv_block1, conv_block3])
    deconv_block2 = deconv_block(merge1, 128, 3)
    merge2 = Concatenate()([deconv_block2, conv_block2])
    deconv_block3 = deconv_block(merge2, 64, 3)
    merge3 = Concatenate()([deconv_block3, conv_block1])
    deconv_block4 = deconv_block(merge3, 32, 3)

    final_deconv = Conv2DTranspose(filters=3,kernel_size=3,padding='same')(deconv_block4)

    dae_outputs = Activation('sigmoid', name='dae_output')(final_deconv)

    return Model(dae_inputs, dae_outputs, name='dae')

denoising_autoencoder().summary()

dae = denoising_autoencoder()
dae.compile(loss='mse',optimizer='adam',metrics=['acc'])

checkpoint = ModelCheckpoint('best_model.h5', verbose=1, save_best_only=True, save_weights_only=True)

hist = dae.fit(train_data_noisy,train_data_clean,validation_data=(test_data_noisy, test_data_clean),
       epochs=200,batch_size=128,callbacks=[checkpoint])

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.show()

plt.plot(hist.history['acc'])
plt.plot(hist.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()


